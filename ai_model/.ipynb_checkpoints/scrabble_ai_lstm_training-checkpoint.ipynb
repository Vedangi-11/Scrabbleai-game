{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ab0a8-2bb8-49a8-b772-7c34735fe935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrabble_ai_lstm_training.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Generate synthetic 15x15 Scrabble-like board data\n",
    "# -------------------------------\n",
    "\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def generate_board_grid():\n",
    "    \"\"\"\n",
    "    Create a 15x15 Scrabble-like grid where each cell is a letter (A-Z) or empty.\n",
    "    We'll encode letters as 1–26 and empty as 0.\n",
    "    \"\"\"\n",
    "    board = np.zeros((15, 15))\n",
    "    for i in range(15):\n",
    "        for j in range(15):\n",
    "            if random.random() < 0.3:  # ~30% filled\n",
    "                board[i, j] = ord(random.choice(letters)) - ord('A') + 1\n",
    "    return board\n",
    "\n",
    "def estimate_score_from_grid(board):\n",
    "    \"\"\"\n",
    "    Synthetic scoring: higher average letter → higher score.\n",
    "    \"\"\"\n",
    "    nonzero = board[board > 0]\n",
    "    if len(nonzero) == 0:\n",
    "        return 0\n",
    "    avg_value = np.mean(nonzero)\n",
    "    filled = len(nonzero)\n",
    "    vowels = sum(chr(int(x) + 64) in \"AEIOU\" for x in nonzero)\n",
    "    return 0.3 * filled + 0.4 * vowels + 0.3 * avg_value + random.uniform(0, 5)\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Create dataset\n",
    "# -------------------------------\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "boards = np.array([generate_board_grid() for _ in range(N_SAMPLES)])\n",
    "y = np.array([estimate_score_from_grid(board) for board in boards])\n",
    "\n",
    "# Normalize input\n",
    "X = boards / 26.0\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps, features)\n",
    "X = X.reshape((N_SAMPLES, 15, 15))\n",
    "\n",
    "print(\"✅ Data shape:\", X.shape, y.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Define LSTM model\n",
    "# -------------------------------\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='tanh', input_shape=(15, 15), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='tanh'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Train\n",
    "# -------------------------------\n",
    "\n",
    "history = model.fit(X, y, epochs=20, batch_size=16, validation_split=0.1)\n",
    "print(\"✅ Training complete.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Save model + tokenizer\n",
    "# -------------------------------\n",
    "\n",
    "model.save(\"model/model_weights_lstm.h5\")\n",
    "print(\"✅ LSTM model saved to model/model_weights_lstm.h5\")\n",
    "\n",
    "tokenizer = {ch: i+1 for i, ch in enumerate(letters)}\n",
    "with open(\"model/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"✅ Tokenizer saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
